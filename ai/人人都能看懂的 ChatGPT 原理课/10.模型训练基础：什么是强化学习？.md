强化学习上一次为大众熟知，还是 2017 年围棋人工智能模型 AlphaGO 打败柯洁的时候。AlphaGo 是由 Google DeepMind 开发的人工智能程序，它使用了深度强化学习算法，能够通过自我学习和对弈经验不断提高自己的水平，它充分展现了强化学习的效果和能力。而 ChatGPT 则将强化学习引入了 NLP 领域，展现出类似人的智能效果。

  


本节主要简单介绍一下强化学习的基本概念，以及它在 NLP 中的建模情况，为学习 RLHF 方法做一个铺垫。

  


# 强化学习基本概念

  


强化学习是一种机器学习方法，旨在让**智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。在强化学习中，智能体根据所处的环境（Environment）中的状态（State），通过执行动作（Action）来影响环境，并从环境中获得奖励（Reward）或惩罚**。智能体的目标是通过学习最大化长期奖励来制定最佳策略。

  


强化学习非常像生物的进化，通过不断地突变基因，由环境来筛选，进而适应环境，生存下来。强化学习的应用非常广泛，主要有游戏、自然语言处理等领域。

  


它的基本建模图如下所示。在超级玛丽奥游戏中，马里奥主人公就是一个由人或模型操控的智能体，游戏的每一个关卡，就是强化学习中的环境。我们玩马里奥游戏的过程，实际上就是一个强化学习的最好例子：从不会玩游戏，通过一次次不断地尝试、失败，最后练成一个马里奥游戏高手，成功通关。这说明了强化学习的本质，是一种 Trial & Failed 学习模式，用中文表达，就是**屡战屡败，屡败屡战，总结经验，获得成功**。用一句俗话来概括，强化学习就是“吃一堑，长一智”。

![](images/image-58.jpg)

  


## 强化学习要素

接下来，我们借助马里奥的例子来解释一下强化学习的几大要素。

-   状态（State）：环境和智能体共同构成的整体状态，这个状态与时间有关，因为每时每刻，状态会变动，智能体可能会改变，环境也可能会变。一般用字母 s 来表示状态，所有时刻的状态构成了一个集合，$$s \in S$$ 这是一个有限状态的集合。

> 在超级马里奥游戏中，玩家每时每刻所处的位置、以及游戏画面中的各种物体和危险障碍都属于游戏的状态。
>
> 在围棋中，棋盘上对弈双方棋子的分布情况，就是围棋当前的状态。

  


-   动作（Action）：智能体可以做出的动作描述，所有的动作构成了一个集合，$$a \in A$$ 是一个有限动作集合。

> 在超级马里奥游戏中，玩家可以通过手柄上下左右的方向键，以及射击和跳跃键，总共 6 个按键来控制马里奥的动作。
>
> 在围棋中，对弈的一方可以把棋子下在棋盘某个位置。围棋棋盘的大小为 19*19，因此，智能体可选择的动作范围最多就是 361。

  


-   策略（Policy）：环境的感知状态到行动的映射方式，$$\pi(s) \to a$$，其含义为，根据当前状态，设计决策函数，采取某种动作。

> 在超级马里奥游戏中，玩家观察到前方有金币（状态），所以按下跳跃键（动作）获取金币，这就是一种游戏操作策略。
>
> 在围棋中，对弈的一方观察棋盘情况（状态），决定把棋子下在哪个位置（动作）。


![10-1.png](images/10-1.png-59.jpg)

-   反馈、奖励（Reward）：环境对智能体行动的反馈。在根据当前一个状态做出决策后，智能体将处于下一个状态，并得到一个反馈值。

> 在超级马里奥游戏中，玩家操作马里奥刚吃完金币（当前状态），按下前进键（动作），刚好撞上前方的乌龟（下一个状态），游戏失败（反馈）就是游戏环境对玩家的一种反馈，也就是惩罚（负奖励）。
>
> 在围棋中，对弈的一方把棋子下在一个正确的位置（动作），直接制胜棋局（反馈），就是围棋对棋手的奖励反馈。


  
![10-2.png](images/10-2.png-60.jpg)


## 一条强化学习路径

现在，我们可以得到一条**状态-策略-反馈**的路径：

$$s_0 \to a_0 \to r_0 \to s_1 \to a_1 \to r_1 \to ... \to s_t \to a_t \to r_t$$

这样一条**路径**也被称作一次**采样**，或一条**轨迹**。

> 以马里奥游戏为例，这条路径实际上就是玩家玩游戏的全过程：
>
> 平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利
>
> 每一次玩游戏都是一次强化学习的试验，在数学上可以看作是一次强化学习的路径采样。在这个例子中，reward 反馈就是玩家是否营救到公主，或者遇到乌龟挂掉。所以，需要经历一条完整的游戏过程才能确定 reward 值。假设玩家成功通关记为 1，失败记为 0，对应到上述标准路径中，每一次的 reward 值都是相同的，都是 1。

  


继续观察这样一条路径，当我们在 $$s$$ 状态时，做下一次动作决策，实际上所依赖的经验完全基于当前的状态。这种只依赖当前状态的特性，就叫做**马尔可夫性**。

> 以马里奥游戏为例，当玩家决定按下跳跃键，以此跳跃陷阱时，之前游戏路径上是否有乌龟，对后续的状态和决策是毫无影响的。

  


## 价值函数

根据前面的基本概念定义，我们可以得到：一个智能体需要不断更新自己的策略函数，以期达到最优的效果。那么如何定义这个效果呢？这里就需要用到价值函数。

> 例如，在一盘围棋中，我执黑棋在某个位置下了一颗子，这个子对于我是否能赢得棋局胜利，究竟有何影响，主要体现在接下来的棋局状况之中。
>
> 换句话说，一条策略的价值，需要衡量它对接下来的操作步骤的影响，设计价值函数也就需要关心策略对未来操作的影响。

### 状态价值函数

状态价值函数（State Value Function）：该函数是针对一个策略 $$\pi$$ 而言的，它是指从状态 $$s$$ 出发，遵循策略 $$\pi$$ 能够获得的期望回报。

$$V^{\pi}(s)=E_{\pi}[G_t|S_t=s]$$

其中：

$$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... + \gamma^T R_{t+T} = \sum^T_0 \gamma^kR_{t+T}$$

首先，状态价值函数 $$V^{\pi}(s)$$ 并不是已经兑现的一条轨迹。它是一个期望值，描述了在状态 $$s$$ 之后，动作轨迹上的累积反馈奖励 $$G_t$$，它把接下来的 $$T$$ 步内的反馈奖励做了一个加权的求和。$$\gamma$$ 是一个随着时间逐渐减退的影响因子，$$\gamma \in (0, 1)$$，其含义为当前的策略更加关注就近的奖励回报，而不太在乎未来步骤的 reward。

> 以马里奥游戏为例：
>
> 轨迹：平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利
>
> 涉及三个状态：平坦的路、前方有乌龟、前方有陷阱。
>
> 涉及三个动作：按前进键、按射击键、按跳跃键。
>
> 营救公主，获取奖励值 1。
>
> 以此我们可以根据计算出 “平坦的路” 这个状态的价值。

  


### 动作价值函数

与状态价值函数相类似，动作价值函数指在当前状态 $$s$$，执行动作 $$a$$ 之后，依旧遵循策略 $$\pi$$ 可以获得的期望回报价值。

$$Q^{\pi}(s,a)=E_{\pi}[G_t|S_t=s, A_t=a]$$

> 假设，小李的手上有 1 万份股票（状态），而股票的价格在未来会波动，其潜在的价值即未来可能的价格的平均值。所谓动作价值函数，即现在小李就把其中的 5000 份股票卖掉（动作），剩余 5000 份获取的潜在价值，当然，也包括小李当下卖掉 5000 份股票直接赚得的钱。

  


动作价值函数和状态价值函数之间有如下关联关系：

$$Q^{\pi}(s,a)=r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi}(s')$$

其中，$$s'$$ 指的是继 $$s$$ 状态之后的状态。动作价值函数，包含了当前动作带来的即时的反馈奖励 reward，以及接下来之后的潜在状态价值。

> 在马里奥游戏中，依据当前状态选择策略，得到的下一状态实际上是确定的。当玩家遇到陷阱，并选择跳跃键后，100% 会成功越过陷阱。因此，在这种**确定性策略**下，两个价值函数的关系可以改为：
>
> $$Q^{\pi}(s,a)=r(s,a) + \gamma V^{\pi}(s')$$

  


当然，强化学习的内涵和外延远远超出上述讲解的基本概念范畴，我们只需要厘清强化学习的本质和思路，方便后续理解 ChatGPT 所采用的做法就可以了。

  


# 强化学习与 NLP 相结合的困难点

  


强化学习最早主要应用在围棋等棋牌游戏以及各种大型电子游戏里，比如，王者荣耀里就有基于强化学习的 AI 人机模式。

强化学习较容易应用在这些场景中，原因就是这些场景都是人为构造的虚拟环境，最终的达成目标也较为简单。换句话说，针对游戏来说，环境容易创造，reward 容易构造。

> 对于超级马里奥游戏而言，游戏软件创造的所有闯关关卡就是其强化学习的环境，而最后的 reward 值是多少，游戏软件会即刻给出玩家是通关或失败。
>
> 对于 AlphaGo 而言，环境就是围棋，围棋棋盘就是它的整个世界，而最后的 reward 值就是判断下棋双方哪一方胜利了，这对计算机来讲就是执行一个程序而已。

  


而想要在 NLP 上使用强化学习，那可就太难了。

  


因为自然语言本质上是一种描述世界的渠道。整个现实世界的事物都可以通过自然语言来表示，形成抽象概念和抽象关系。**因此，NLP 所依赖的环境是整个现实世界**，整个世界的复杂度远远不是一个 19 乘 19 的棋盘可以比拟的。同时，NLP 领域无法设计 reward 函数。在 ChatGPT 创造出来之前，没有任何一个计算机程序，能够对一个 NLP 程序输出的结果给出准确的优劣判断。在 NLP 领域，reward 值只能由人工一个个给出。

  


> 为 NLP 模型构建 reward 函数，有点像鸡生蛋、蛋生鸡的死循环。
>
> 我们希望能够通过强化学习的方式，制作出一个先进的、通过图灵测试的语言模型。
>
> 为了实现强化学习，我们需要一个先进的、通过图灵测试的语言模型来做 reward。

  


# ChatGPT 与强化学习

介绍完强化学习的基本概念和建模形式之后，我们用两个例子（马里奥和围棋）介绍了一下强化学习如何应用在具体的任务中。大家可以举一反三，试想一下，强化学习应该如何应用在自然语言处理中。

  


在围棋中，棋盘就是环境，有很多电子围棋程序能够自动计算出下棋双方，哪一方胜利，哪一方失败。马里奥游戏同理，判断游戏失败的条件非常简单，碰到危险物体，掉进沟里，只需要写一段简单的程序即可。根据强化学习“吃一堑长一智”的学习原则，只有当智能体接收到胜利或失败的反馈信息后，才能够收获智能。

  


而在自然语言中，ChatGPT 作为一个智能体，其模型的优化也必须建立在，有一个程序或人，来告诉 ChatGPT，你模型给我生成的输出内容究竟是好，还是不好。若是生成的不好，那么 ChatGPT 就拿着这个负反馈回炉重造，以此达到“吃一堑长一智”的学习原则。

## ChatGPT 的强化学习概念映射

我们先来回顾一下 ChatGPT 的工作流程：



![10-3.png](images/10-3.png-61.jpg)

在上文的强化学习介绍中，我们以马里奥游戏为例进行介绍，如何闯关，如何胜利。其中涉及一个时间的概念，即马里奥的前进过程中，始终有一个状态随时间变化的问题。

而 ChatGPT 模型建模上并没有时间这个概念，因此，ChatGPT 以强化学习形式进行建模，对应的强化学习要素有如下变化：

-   智能体：即 ChatGPT 模型本身；
-   环境：整个现实世界，被自然语言所描述和抽象，ChatGPT 实际是和人交互，因此，对于 ChatGPT 而言，人类用户就是它的环境；
-   状态：在 ChatGPT 当中，根据 ChatGPT 输入输出流程图可知，状态就是 prompt。此时，状态不再依赖时间存在（像马里奥游戏那样），时间被省略掉了，建模形式是无时间状态的；ChatGPT 实际上只关注输入和输出，也就是，我们只关心这一次的反馈就好，不需要像马里奥游戏一样再关心下次动作和反馈；
-   策略、动作：我们知道，强化学习中策略（Policy）实际上就是一个对环境响应的概率分布，而 ChatGPT 本身就是一个大语言模型概率分布（第 3 节中介绍）。因此，ChatGPT 模型对给定的输入，反馈一条输出文本，就是按照当前模型策略，进行了一次动作；
-   反馈 reward：人对于 ChatGPT 输出结果的评价，好或者差；注意，在上一节中，我们提到了，人工标注数据的代价非常大。这也是 reward 难以制做之处。

  


## 制作 reward model

ChatGPT 应用强化学习的主要困难，就是**模型给出的输出反馈，没有什么方便的程序或者机制给出恰当的评价，就只能靠人工一个个的反馈**。

OpenAI 还是财大气粗，愿意花钱做这些看起来没技术含量的工作，公司找了 40 个外包，标注了大量的数据。利用这些标注数据，制作了一个 reward model，一举解决了设计奖励函数的问题。

  


# 总结

-   强化学习是让智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。
-   NLP 与强化学习结合的难点在于环境复杂（现实世界事物无限多），reward 函数难设计（只能靠人工评价模型输出的好坏）。