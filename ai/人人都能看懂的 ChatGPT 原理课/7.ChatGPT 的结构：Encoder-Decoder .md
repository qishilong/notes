前面第 5-6 节介绍了 Self-Attention 自注意力机制、Transformer 模型结构。这就相当于我们盖房子准备好了砖头，本节主要介绍如何把 Tranformer 模型组合起来，形成一个完整的 GPT 模型结构，而组合的方式，就用到了Encoder-Decoder 编解码架构模式。在这一节中，我们将对前述章节的内容做一个汇总，让读者对 GPT 模型从全局有一个清晰的认知。

# Encoder-Decoder 编码器-解码器架构

在第 1-2 节中，我们大致介绍了语言模型的编解码结构。如下图所示。


  
![7-1.png](images/7-1.png-38.jpg)


实际上，encoder-decoder 这一套模型架构最早是用于解决机器翻译问题的，感兴趣的读者可以读一下这篇经典论文【2014：[Neural Machine Translation by Jointly Learning to Align and Translate](https://scholar.google.com/scholar?q=2014+-+Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate&hl=en&as_sdt=0&as_vis=1&oi=scholart)】。机器翻译模型接收一条英文语句，然后经过模型的一番操作，最后输出一条对应的中文翻译结果。这种建模最早被称为 seq2seq，其含义为 sequence to sequence，即序列到序列，输入一条文字序列，输出一条文字序列。

此后，在 NLP 领域，seq2seq 可以被应用于各种各样的文字序列任务上，也就成了 NLP 领域的一种标准建模方式。


![7-2.png](images/7-2.png-39.jpg)

在 encoder-decoder 这种建模方式中，encoder 像极了一个人接收文字信息思考的过程，decoder 则像极了一个人将大脑中的信息转换成语言表达出来的过程。可以说，encoder-decoder 就是一种机器模拟人脑思考的方式。

  


另外，encoder-decoder 并非指一种具体的模型结构，它是一种宽泛的模型**设计架构**。

-   这种编解码结构不仅仅局限于 NLP 领域，它也可以应用于图像处理、音频处理等领域，例如图像领域的对抗生成模型 GAN 等。
-   我们知道，GPT 采用了 decoder 架构，丢弃了其中的 encoder 部分，其中的具体结构是 Transformer。但是，编解码架构的内部设计还可以采用循环神经网络（ RNN ）这种模型结构。当然，如果未来有更好的设计，也可以替换为别的具体模型结构。

  


> 如果说 GPT 是一幢房子，Transformer 是盖房子的砖头，那么，encoder-decoder 模型架构就是房子的具体结构，如门朝哪开，有几间卧室等等。除了使用 Transformer 这种砖头之外，还可以使用 RNN 等木头来搭建同样结构的房子。
>
> 当然，随着大模型的发展，实践证明，Transformer 这种结构，不论从计算性能还是适用性来说，都比 RNN 模型要强。

  


## GPT 中的编解码架构

接下来，让我们来绘制一下，Transformer 是如何嵌入 GPT 的 encoder-decoder 架构中的。如下图所示，Transformer 模型结构中省略了 norm 正规化、残差计算和 dropout 模块。


  
![7-3.png](images/7-3.png-40.jpg)


当用户输入了`掘金社区是一个便捷的技术交流`这条语句时，模型首先结合第 4 节中介绍的 embedding 词嵌入，将文本转换为 token，进而找出对应的 token embedding 和 position embedding（本例中不需要 segment embedding），将两者相加即可进入 Transformer 结构做注意力计算。

  


Transformer 结构本身可以有很多层，每一层的输入 tensor 和输出 tensor 维度大小全部相同，前一层 Transformer 的输出就可以作为下一层的输入，像罗列方块积木一样。直到最后一层。如下图所示，这里省略了 Transformer 内部的结构，展示了三层 Transformer 结构。


![7-4.png](images/7-4.png-41.jpg)

假设最后一层的 Transformer 输出了 $$M \times N$$ 维的 tensor，如上图中的紫色部分，其中$$M$$表示 token 的个数，$$N$$表示每个 token 的维度，同时假设在 BPE 算法的 token 词表中，总共有 $$K$$个 token。那么，最后一层线性层可以设计为$$M \times N \to K$$的一个函数映射，得到一个 $$K$$ 维向量，如上图中的黄色部分。一般来讲，这个向量的维度可能达到几万维到几十万维不等，**它的长度和词表中有多少 token 是相等**的。

  


对这个 $$K$$ 维向量进行**解码**，就可以得到输出的结果。我们来具体讲讲如何解码。

  


## 解码得到输出 Token

### 贪婪搜索 Greedy Search

假设，我们根据 BPE 算法（第 4 节）得到一份词表，按顺序，其中第 6 个 token 为 `平` 字，第 8 个 token 为`网`字。

| 1: 我 | 2：中 | 3：上 | 4：这 | 5：天 | 6：`平` | 7：司 | 8：`网` | ... |
| ---- | --- | --- | --- | --- | --- | --- | --- | --- |

根据第 5 节中介绍的 softmax 函数，可以对计算的 $$K$$ 维向量做一个 softmax，得到一个长达 $$K$$ 维的概率分布，假设具体数值如下所示：

| 0.0016 | 0.1120 | 0.0011 | 0.00006 | 0.0015 | `0.3410` | 0.0102 | `0.2179` | ... |
| ------ | ------ | ------ | ------- | ------ | ------ | ------ | ------ | --- |

从中可以看到，概率值最大的一个数字是 0.3410，它是这个 $$K$$ 维向量的第 6 个数字。因此，我们就可以从 token 词表中选择第 6 个 token，即 `平` 字作为模型输出的结果，整体句子就成了 `掘金社区是一个便捷的技术交流平`。这种根据向量最大值寻找对应索引的操作叫做 $$argmax$$。**按照最大概率值选择模型输出的 token**，这种方法叫做**贪婪搜索（Greedy Search）** 。

  


既然我们得到了一个维度长达 $$K$$ 维的向量，所有向量值相加为 1，那么我们可以对 token 词表进行采样。

  


> 所谓**采样**，简单理解就是掷骰子。我们都知道，一颗方形骰子有 6 个面，分别代表 1，2，3，4，5，6 几种选择。每次投掷，得到的结果是一次采样，每次的投掷结果均不同，每一种结果命中率都是六分之一。
>
> 而在 ChatGPT 模型输出结果时，也以上述采样的方式，按照每个 token 对应的命中概率值进行随机抽取，只不过，可选择范围包含了 token 词表中所有的 token。这就说明了模型输出的结果具有随机性，并非每次都相同。

  


按照贪婪搜索的方式，实际上是取消了根据 token 概率分布做采样的操作。我们知道，有 0.3410 的概率输出得到`平`字，这个结果没错；但也有 0.2179 的概率模型会输出得到`网`字，这个值也很高，若模型输出 `网`字，语言读起来完全能说得通，并不能算作错。

因此，贪婪搜索是有一定缺陷的，即人为地漏掉了一些正确的答案。

  


### 束搜索 Beam Search

为了克服贪婪搜索会漏掉一些概率值稍低的正确答案这个缺陷，可以预先选择一个范围。比如，我们把采样范围扩大，选择结果中概率值最大的两个索引位置，即 0.3410 和 0.2179，其余的概率值全部不考虑。仅从这两个概率对应的 token 中进行筛选，那选择 token 的概率分别为：

$$p(平)=\cfrac{0.3410}{0.3410 + 0.2179}=0.61$$

$$p(网)=\cfrac{0.2179}{0.3410 + 0.2179}=0.39$$

由此，我们可以从这套概率分布中二选一，0.61 的概率可以抽取出 `平` token，0.39 的概率可以抽取出 `网` token。

  


然后，我们可以继续迭代，让模型输出`平台`二字，或`网站`二字，这两种答案都是正确的。这种先选择最高概率的若干选项，再在其中随机抽取的方式叫做**束搜索（Beam Search）** ，所谓 Beam 就是指一束光中，光线不止一条，而是有多条，对应在解码中，就是指有多个选择，它可以克服贪婪搜索选择范围仅仅只有 1 个 token 的缺点。

  


要选择多少个备选 token 进行采样，需要预先人为设定一个数量 $$k$$。可以看出，若此值越小，生成的结果越固定，反之结果越灵活多变。

  


### 核搜索 Nucleus Search

在 Beam Search 中，选择多少个可选 token 也有一定的策略，例如，上面我们设定了只选择概率值最大的 2 个值。或者换个思路，我们可以设定，把模型输出的 $$K$$ 维概率分布值按从大到小的方式排列，若前 T 个概率值加起来大于 top_p（介于 0~1 之间的值），则以这 T 个值作为最终的抽取范围。

  


以上面为例，假设预先设定$$top\_p=0.6$$，而分布中，概率值最大的三个相加可得：
$$0.3410 + 0.2179 + 0.1120=0.6709 > 0.6$$。

因此，我们就从第 2，6，8 三个 token 中，按照概率进行 token 抽取。

这种输出方式被称为 **top_p 搜索，也叫核搜索（Nucleus Search）** ，是 Beam Search 的一种变体。当预先设定的 $$top\_p$$ 值变大时，可用于选择的 token 数就变多，模型生成的结果就更加多变，不可靠；当 $$top\_p$$ 变小时，可选择的 token 数就变少，模型生成的结果就更加固定；当 $$top\_p$$ 为 0 时，核搜索退化为 Greedy Search。

  


在这个例子中，`平` token 的概率值是 0.3410，`网`token的概率值是 0.2179，剩下的 token 占据了很大的概率值，但就不再是正确答案了。这实际上也存在一定的缺陷，即正确目标的概率值尽管是最大的，但是数值本身仍然偏小。

  


### 温控搜索 Temperature Search

面对上面的问题，我们得设计一种思路，能够在保证各个 token 的概率值相对大小排序不变的情况下，调整其概率值。例如，`平` token 的概率值比`网`大，这个大小关系是确定的，但希望`平` 的概率值变得更大一些。

  


回忆一下第 5 节中对 softmax 函数的介绍，在上图中，softmax 函数的输入是一个 logits，输出是对应的每个 token 位置的概率值。这里的 logits 就可以理解为对数化的概率值，设 $$u$$ 是 logits 向量，$$p$$ 是对应的概率值，那么两者之间的关系为：$$p_i=\cfrac{exp(u_i)}{\sum_j exp(u_j)}$$。

这就是 softmax 公式。所谓**温控搜索（Temperature Search）** ，就是对上述公式做一个调整，加入一个参数 $$T \in (0,1)$$，得到：$$p_i=\cfrac{exp(\cfrac{u_i}{T})}{\sum_j exp(\cfrac{u_j}{T})}$$。

以`平`字为例，当$$T=1$$时，其对应的概率值是 0.3410，而当 T 值逐渐变小后，其对应的概率值会逐渐变大，模型对`平`字抽取的概率也就变大，模型的输出结果也就更加固定。相反，T 值逐渐趋近于 1 时，模型搜索结果也就变得更加灵活多变，不可控制。

  


> 这种现象非常像物理热力学中的现象，当温度升高后，分子热运动加剧，运动变得无规则不可控；而当温度降低后，分子热运动减弱，从宏观上看，运动趋于稳定。

  


ChatGPT 的解码方法，就是温控搜索和核搜索的一种结合体。一方面，从全局 token 中选择一个范围（核搜索），另一方面，条件这个范围内的温控参数（温控参数），使得采样不同 token 的值出现变化。如果读者尝试过调用 openai 的接口，就会看到这个参数选项值。

![](images/image-42.jpg)

最后，需要提一下，模型解码是个循环过程，即根据前一个 token，输出后一个 token，反复可以迭代进行。那么，这个循环什么时候被打破？换句话说，模型什么时候输出完呢？

其实，在 token 词表中，单独设置了一个特殊符号 token`<eos>`，意指 end of speech，当模型预测输出了这个 token，那么循环就终端了，ChatGPT 也就会认为，整个要输出的答案完成了。

  


## GPT 是一个解码器

从上面的整个计算流程中我们可以看到，模型首先接收一串输入数据，经过 Transformer，这属于 encoder 部分，末尾模型输出预测的 token 字符，这属于 decoder 部分。整个流程和本节第一个流程示意图中略有差别。主要差别在于，前述介绍的编解码架构中，编码器和解码器相互独立，中间有一个单独的信号相连。而在 GPT 模型中，实际上编解码融为一体。


![7-5.png](images/7-5.png-43.jpg)


下图是 Transformer 论文中【2017 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)】的最原始的编解码模型架构，其中，左半边为编码器，右半边为解码器。而前述的 GPT 模型架构，其实就是原始结构的右半边。因此，我们也常说，GPT 系列模型仅仅使用了 decoder 部分。

![](images/image-44.jpg)

  


  


另外，读者可以对比上图最原始的 Transformer 的内部结构和第 6 节中 GPT 结构的异同。如果仍不清楚，可以返回上一节再阅读一下。提示一下，主要包括稀疏操作、norm 操作等等。

  


# Mask 掩码层

在实际的模型计算过程中，读者可能已经使用过 ChatGPT，模型输入和输出的句子有长有短，长则几千个字，短的不足 10 个字，始终处于变动。而 ChatGPT 模型结构则相对固定，会预先设置允许一个模型接受的固定最大 token 序列长度。假设模型可以接收的最大 token 数量为 20 个，而用户输入的数据只有 `掘金社区是一个便捷的技术交流`这 14 个 token 时，模型会对这部分数据做一定处理，如下图所示。

![7-6.png](images/7-6.png-45.jpg)
  


图中展示了模型输入 token embedding 的整体结构，它规定了模型的输入最大 token 数为 20。当用户的输入低于 20 个 token 时，模型会自动为用户的输入补齐，添加一个特殊的`<pad>` token（图中以`/p`表示，它和`<eos>`token 性质是相同的，并不是表示具体的某个字符，而是仅仅起到一种功能上的作用），它本身不具有实际意义，仅代表占位符；当用户的输入超过 20 个 token 时，模型又会将超出的部分截断，仅保留前 20 个 token，形成一个不完整的输入。

  


模型在进行后续的 Transformer 操作时，需要考虑`<pad>` token 本身不代表任何含义，这些字符不应该参与上下文的相关性自注意力计算。为了解决这个问题，**Mask 掩码层**被提出。

  


> Mask 广义来讲是一块蒙版，指覆盖在图像上，遮挡部分景物。
>
> 如下图中，用一块白色的蒙版，遮住图片中的部分内容，就只能看见猫咪的一只眼睛。

![](images/image-46.jpg)

Mask 概念同样可以运用在 Transformer 中。根据第 5 节的介绍，自注意力机制主要采用$$Q, K, V$$三种向量进行计算。如果考虑 Mask 蒙版，假设 ChatGPT 模型仅输入了`掘金`两个 token 字符，预测接下来应该填写的字符，则它将变为如下形式：


![7-7.png](images/7-7.png-47.jpg)

在上图中，mask 层为每一个 token 位置设置了一个 1，0 取值，若设为 1，则对应位置 token 正常进行计算，而设为 0 时，对应位置不参与注意力计算。最终的 softmax 概率值为 0。

  


根据第 6 节中的介绍，ChatGPT 采用了稀疏自注意力机制，根据跨步分解（Strided Factorized）和固定分解（Fixed Factorized）两种方式屏蔽掉某些 token 位置的注意力计算，采取的也是 Mask 的方式。

  


# 总结

-   ChatGPT 模型基于 encoder-decoder 模型架构进行建模。
-   ChatGPT 模型采用**核搜索、温控搜索**结合的方式生成输出结果，并基于 temperature 调节生成结果的随机性，值越大，随机性越强，值越小，生成的内容越固定。
-   ChatGPT 主要采用 **Mask 掩码**的方式，屏蔽掉不参与注意力计算的 token 位置。