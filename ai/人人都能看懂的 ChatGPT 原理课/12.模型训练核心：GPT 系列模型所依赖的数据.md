在前面的章节中，我们从模型结构、训练方法层面讲解了 ChatGPT 的原理，通读下来，大家应该可以对 ChatGPT 的原理有一个大致的认识和了解了。

> 如果把学习 ChatGPT 原理比作学习烹饪的话，那么，学完前面章节的模型原理就相当于学会了一份菜谱。
>
> 可是，学会菜谱可并不算学会了烹饪，还需要了解食材怎么选取，烹饪的厨具怎么选择。
>
> ChatGPT 模型的食材就是数据，厨具就是算力。

ChatGPT 能够取得如此卓越的效果，依靠的绝不仅仅是模型结构和算法原理创新，**数据**和**算力**也是其中极为重要的两环。自从 ChatGPT 发布公测以来，不断有其它公司或机构宣称也制作了性能可以对标 ChatGPT 的模型，但普遍来讲，都不及 ChatGPT 的效果优秀。换句话说，数据和算力在一定程度上卡住了后来者的脖子。

然而，OpenAI 机构并未公开 ChatGPT 相关的训练数据集和所耗用的算力。因此，我们只有通过回顾 GPT 系列模型公开的信息，尝试使用 ChatGPT，来大致分析 ChatGPT 的数据特点和规模。**数据的准备、收集、清洗**对于训练一个优质的模型来说非常重要。

## GPT 初代训练数据与规模

GPT 初代沿着**语言模型预训练**（第 3 节）+ 特定任务 **finetune** 的思路展开。

*   其中，语言模型预训练所使用到的数据主要是 **BooksCorpus** 数据集，其中包含了 7000 篇各种语言风格的英文图书，篇幅较长，适合于模型训练较长的上下文依赖。数据集总计大约 5GB。
*   特定任务 finetune 则采用了在 NLP 学术界经常使用的测评数据集，如下表所示。

![](images/image-70.jpg)

表中左侧分别是传统的 NLP 建模类型自然语言推理、问答系统、句子相似度计算、文本分类。右侧为数据集名称，读者可以根据名称搜索到相应的数据集。

> 在 GPT 初代模型中，它已经运用了在当时看来规模庞大的数据集和模型规模。但是，BooksCorpus 数据集总共只有 7000 本图书，这个数据量还是比较少的，内容也比较单一。

与此同时，GPT 模型使用的参数量规模大约在 **1.17 亿**，这个量级还是比较小的。

## GPT-2.0 训练数据与规模

GPT2.0 十分注重数据的质量，从一个国外社交媒体 **Reddit** 上爬虫爬取了大量高质量数据，经过精心的组织和整理，形成了一个总数据量达 **40GB 的数据集 WebText**。社交媒体上的信息包罗万象，这些数据集的覆盖领域、内容、语言种类、风格、数据规模，就比 BookCorpus 要广得多了，因此它比较适合用于训练多任务模型。

> OpenAI 在发表的 GPT2.0 论文中着重强调，**在训练模型过程中，只有高质量的数据才能让模型获得好的效果，低质量的数据只会导致模型学习到垃圾信息。** 也就是俗话说的，种瓜得瓜，种豆得豆。

此外，模型已经丢弃了预训练+ finetune 的思路，研究者们为了解决多任务学习的问题，直接预训练模型，然后不改变模型的参数和结构，后接 zero-shot，初次在多种 NLP 任务上做了尝试。

## GPT-3.0 训练数据与规模

紧接着，GPT-3.0 采用了更大的数据，并在 few-shot 的道路上继续前进。它所采用的模型结构和数据情况与 ChatGPT 已较为接近。

GPT3 的主要数据集来源于 **Common Crawl（CC）** ，它是一个开源的互联网爬虫，旨在为 AI 提供充足的数据集。它的数据规模非常大，达到了 570 GB。语料分布极为广泛，只要是能够被爬虫爬到的，不限语言种类、时间、内容，统统被囊括进来。

当然，直接利用这套数据训练模型，会有一些低质量的文本对模型效果产生影响。所以，GPT3 做了一些操作，清洗文本，提升数据质量：

*   制作了一个小型的高质量数据集，比较其与 Common Crawl 中的所有数据集的数据质量，把低质语料过滤掉。

*   Common Crawl 中还有很多的重复数据，GPT3 做了一个简单的去重操作（注意，570GB 是去重和清洗过后的数据量）。

*   再加入一些其它的数据集，增加数据的多样性。那么，到底加入了哪些数据集呢？

    *   WebText：这是 GPT2 中的数据集，再次利用了；
    *   Books：这是 GPT 初代的数据集，又再次扩充利用了；
    *   Wikipedia：维基百科中囊括了大量的高可信度的文本和知识，它有助于提升模型对事实类问题的回答。

所有利用的数据集以下表展示，这几种数据集，表中是以第 4 节中介绍的基于 BPE 算法做了 tokenization 之后的 token 为计数单位的。最后一列可以看作是这几个数据集的重要性。数值越高，说明数据集的质量越高，内容越可信，值得模型多学习几遍。数值越低，则说明数据集可信度较低，学或不学，并不起关键作用。

![](images/image-71.jpg)

模型随着数据量的上升，其参数规模势必要跟着扩增。GPT3 的参数规模已经达到了 **1750 亿**。相比于前几代模型，GPT3 已经是个庞然大物。

> 说个不太科学的类比，人类之所以统治地球，站在生物链的顶端，可能并不是因为人的大脑结构比其它动物高级，而是人脑中的神经细胞数量远远碾压其它动物。

## InstructGPT 训练数据与规模

GPT3、InstructGPT、GPT3.5、ChatGPT 这几个模型的关系如下图所示：

![12-1.png](images/12-1.png-72.jpg)

InstructGPT 模型直接在 GPT3 模型的基础上，尝试了 RLHF 训练方法。ChatGPT 实际上是在预训练语言模型 GPT3.5 基础上，结合 RLHF 训练方法得到的产物。

### 数据集数量

根据第 11 节对 RLHF 原理的介绍，InstructGPT 制作的训练数据集数量情况如下表所示：

![](images/image-73.jpg)

其中，prompt 就是要输入给模型的指令，主要包括数据标注员自己想出来的，也包括从互联网上用户那里收集到的。SFT 即指有监督学习，RM 为训练奖励模型，PPO 即强化学习。

在 SFT 阶段，标注员提供的数据量多过用户提供的；而在 RM 阶段，用户提供的数量多过标注员；在强化学习阶段，已经完全脱离了标注员的标注数据。依据我个人粗浅的理解，标注员人数是有限的，因此构造的 prompt 语言风格、指令内容多多少少会有一定的规律，而互联网上用户的提问形式更加多样，内容覆盖也更广，在 PPO 阶段学习，使用了完全针对用户的数据集。

总体而言，这个数据量大约几万条，相比海量的预训练语言模型中用到的文本语料而言，可以说微乎其微，它说明了 **RLHF 在少量的数据量上依然可以取得非常惊艳的效果**。

### 数据集分布

所有的这些 prompt 类型分布如下表所示，绝大多数都是文本生成、头脑风暴、聊天、总结等偏主观的指令。换句话说，模型学习到了更多的语言知识和语言操作，而对常识知识、事实性知识较为缺少。

![](images/image-74.jpg)

> 例如，写一个爱情剧本，写一个简短的故事，翻译一段外文，头脑风暴一下，这些都更加偏向文学题目，而事实性、常识性的内容如计算 23+194.9=?，世界上第二高峰叫做什么名字？等等客观题目则比较缺乏。
>
> 这就很容易导致模型出现胡言乱语、编造事实的情况出现，这被称为**幻觉妄语（Hallucination）** ，仿佛一个精神病人在说一些不合逻辑和事实的事情。

在上表中，真正针对模型对客观知识性回复的内容，只有**开放域问答（Open QA）** 这一个子类别。这个类别仅仅占到总数据量的 12.4%。所谓开放域问答，即问题的答案是客观的，但答案的搜索范围不设限。实际上，互联网上用户针对 ChatGPT 的问答，开放域问答占绝大多数，例如以下 prompt：

![](images/image-75.jpg)

相对应的则是**闭域问答（Closed** **QA** **）** ，类似于语文考试中的阅读理解，答案在给出的文本内部，但是文本中的信息真实性不可考证。例如以下：

![](images/image-76.jpg)

假设我们给 ChatGPT 提供外部的文本，以 prompt 形式输入进去，然后针对文本内容进行闭域问答，实际上就构成了一个垂直领域的问答系统，这个目前已经有了很多的应用产品，如 [ChatPDF](https://www.chatpdf.com/)、[langchain](https://langchain.com/) 等等，大家感兴趣可以搜索一下它们的实现原理。

### 数据集标注标准

在第 11 节中，详细介绍了 RLHF 算法过程，其中讲到了在制作 Reward 模型时，需要对模型根据 prompt 生成的response 进行打分，而这个打分实际上需要有一定的标准。那么，OpenAI 设计了如下的打分标准：

![](images/image-77.jpg)

在这份标准中，主要包括回答偏题、胡说八道，包含色情、暴力内容，给出有害建议，随意道德评价等等。例如，

> prompt2：我想去重庆玩，有哪些好玩的地点呢？
>
> response：你怎么这么无知呀，连重庆哪里好玩都不知道。重庆是一个充满活力和美食的城市，有很多值得一去的地方。以下是一些推荐：春熙路步行街，都江堰水利工程，华美极乐世界，少林寺。

其中，“华美极乐世界”是一个完全不存在的景点，这就属于胡说八道（Hallucination）的类型。“你怎么这么无知呀”则属于对用户产生了不恰当信息这一类型。

由此，OpenAI 在数据标注时，着重强调了 ChatGPT 模型给出的回答应当满足几个原则，即**对用户有帮助（helpful）、符合事实没有胡编乱造（truthful）、不存在偏见和歧视（harmless）** 。

## GPT-3.5 训练数据猜测

尽管没有可靠资料，但是我们依然可以推断出，GPT3.5 利用了更加庞大的数据集，并且在模型训练过程中，事实性、可靠性的、包含客观知识的数据集被赋予了更高的权重。

而小说、故事、人们的聊天等等内容，则往往偏向人为编造。这部分数据集的作用在于，让语言模型学会丝滑地操作语言、理解语言。事实上，ChatGPT 在语言文字的运用方面已经超越了绝大多数人类，并不需要再做额外的训练。

此外，**GPT3.5 已经超越了自然语言处理这个概念**。自然语言通常就指人类使用的英语、汉语、日语等。而从公测的模型能力来看，它完全可以读懂如 C、Python 等程序语言，这些是非常抽象的，也可以读懂计算机的二进制数值的功能含义。

> 用户：请问以下二进制数据是什么含义？101010000011101001010010010000011110000111。
>
> 模型：上述二进制信息代表了……

此外，以文字形式表示的图像也可以理解，尽管图像是一种二维数据，文字是一维的串行数据：

> 用户：请问以下图片数据是什么含义？
>
> 　　　`*`
>
> 　　`*****`
>
> 　`*********`
>
> `*************`
>
> 模型：上述图像代表了一个三角形。

换句话说，但凡能够以文字形式表示的任何数据，GPT3.5 都可以处理并理解。

## ChatGPT RLHF 训练数据猜测

在 InstructGPT 模型中，RLHF 强化学习的训练阶段，仅仅使用了几万条数据集，这个数据量级实在是太低了。从GPT 系列模型的发展规律来看，优质的模型效果 100% 来源于充足、高质量的数据集。自从 ChatGPT 接口放开以后，OpenAI 收获了来自全世界各种不同国家、语言、文化的问答数据。因此，我相信 OpenAI 用了比几万条数据集高出量级的 prompt 来制作 ChatGPT的 RLHF 部分。

而且，ChatGPT 的模型能力不仅仅是完成一些语言任务，还具备了编程、从程序中找bug、计算数学、回答物理、化学、生物等等其它科学客观问题。我想，ChatGPT 在制作 prompt 过程中，也加入了占比更高的客观问题 prompt。

# 总结

*   GPT 系列模型的发展伴随着数据量的指数级增长，证明了只有充足参数的大模型才能具备较高的智能。
*   InstructGPT 以较少的数据集训练得到了超过 SFT 的效果，证明了 RLHF 方法的有效性。
*   数据的质量高低对模型训练的效果影响很大，需要克服幻觉妄语（Hallucination）。
*   ChatGPT 模型已经超越了自然语言处理的范畴。
